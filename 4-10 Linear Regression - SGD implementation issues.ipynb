{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch SGD\n",
    "- 셔플링 먼저 시켜준다는 것 알고있기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoches): # 전체 Epoch가 iteration 되는 횟수\n",
    "    X_copy = np.copy(X)\n",
    "    if is_SGD: # SGD여부 -> SGD일 경우 shuffle\n",
    "        np.random.shuffle(X_copy)\n",
    "    batch = len(X_copy) // BATCH_SIZE # 한번에 처리하는 BATCH_SIZE\n",
    "    \n",
    "    # DO weight Update\n",
    "    for batch_count in range(batch):\n",
    "        X_batch = np.copy(X_copy[batch_count*BATCH_SIZE :\n",
    "                         (batch_count+1)*BATCH_SIZE]) # BATCH_SIZE크기만큼 X_batch생성\n",
    "    print(\"Number of epoch : %d\" %epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_regression\n",
    "X,y = make_regression(n_samples = 1000,\n",
    "                     n_features = 1,\n",
    "                     noise = 10,\n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.75873949],\n",
       "       [ 1.03184454],\n",
       "       [-0.48760622],\n",
       "       [ 0.18645431],\n",
       "       [ 0.72576662],\n",
       "       [ 0.97255445],\n",
       "       [ 0.64537595],\n",
       "       [ 0.68189149],\n",
       "       [-1.43014138],\n",
       "       [ 1.06667469]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergencee process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_lr = linear_model.LinearRegressionGD(eta0=0.001, epochs=10000,batch_size=1,shuffle=False)\n",
    "bgd_lr = linear_model.LinearRegressionGD(eta0=0.001, epochs=10000,batch_size=len(X),shuffle=False)\n",
    "sgd_lr = linear_model.LinearRegressionGD(eta0=0.001, epochs=10000,batch_size=1,shuffle=True)\n",
    "msgd_lr = linear_model.LinearRegressionGD(eta0=0.001, epochs=10000,batch_size=100,shuffle=True)\n",
    "# eta0 : 첫번째 learning rate\n",
    "# epochs : 전체 iteration이 몇번 돌것인지\n",
    "# batch_size : 한 번에 몇개씩 들어갈 것인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efor epoch in range(epoches): # 전체 Epoch가 iteration 되는 횟수\n",
    "    X_copy = np.copy(X)\n",
    "    if is_SGD: # SGD여부 -> SGD일 경우 shuffle\n",
    "        np.random.shuffle(X_copy)\n",
    "    batch = len(X_copy) // BATCH_SIZE # 한번에 처리하는 BATCH_SIZE\n",
    "    \n",
    "    # DO weight Update\n",
    "    for batch_count in range(batch):\n",
    "        X_batch = np.copy(X_copy[batch_count*BATCH_SIZE :\n",
    "                         (batch_count+1)*BATCH_SIZE]) # BATCH_SIZE크기만큼 X_batch생성\n",
    "    print(\"Number of epoch : %d\" %epoch)\n",
    "    \n",
    "# 이 코드로 위의 경우가 모두 계산 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD : 랜덤하게 값을 주기때문에 값이 튀게 됨\n",
    "- iteration이 가장 작았던 것은 GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gd_lr.fit(X,y) # Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit bgd_lr.fit(X,y) # Full-batch Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sgd_lr.fit(X,y) # Stochstic Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit msgd_lr.fit(X,y) # Minibatch-SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한점씩 들어간 GD, SGD가 조금 더 시간이 오래 걸림\n",
    "- BGD가 제일 빠름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate\n",
    "- msgd : 줄어들긴 하지만 심하게 떨림\n",
    "- bgd : 서서히 줄어듦\n",
    "- gd,sgd는 어느정도 수렴함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 많이 쓰는 것은 msgd이지만, 데이터가 적으면 gd나 bgd가 좋음 (사실 normal equation을 쓰면 됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate는 일정해야 하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning-rate decay\n",
    "- 일정한 주기로 Learning rate를 감소시키는 방법\n",
    "- 특정 epoch마다 Learning rate를 감소 (epoch 설정 가능)\n",
    "- self._eta0 = self._eta0 * self._learning_rate_decay\n",
    "- Hyper-parameter 설정의 어려움\n",
    "- 지수감소, 1/t감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종료조건 설정\n",
    "- SGD과정에서 특정 값 이하로 cost function이 줄어들지 않을 경우 GD를 멈추는 방법\n",
    "- 성능이 좋아지지 않는/필요없는 연산을 방지함\n",
    "- 종료조건을 설정 - tol > loss - previous_loss\n",
    "- tol은 hyperparameter로 사람이 설정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
